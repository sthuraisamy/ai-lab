{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_token_replace_vals = [(re.compile(r'@\\w+'), ''),\n",
    "                          (re.compile(r'http\\S+'), '')]\n",
    "stop_words_filename = 'english'\n",
    "additional_stop_words = ['amp', 'rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_set(data_filename):\n",
    "    '''function to read dataset and print some information about the dataset'''\n",
    "    # read csv file into dataframe\n",
    "    data_df = pd.read_csv(\n",
    "        data_filename, delimiter=\",\", encoding=\"utf-8\")\n",
    "\n",
    "    # print the info of twitter data frame\n",
    "    print(data_df.head())\n",
    "    print(data_df.shape)\n",
    "    print(data_df.columns)\n",
    "    print(data_df.isnull().sum())\n",
    "\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_words_filename, additional_stop_words):\n",
    "    '''function create english language stop words and add additional required terms'''\n",
    "    stop_words = stopwords.words(stop_words_filename) # get stop words for given language\n",
    "    stop_words.extend(additional_stop_words) # add additional stop words\n",
    "\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_token_cleanup(text, replace_vals):\n",
    "    '''function to pre-process the tweets'''\n",
    "    text = text.lower()  # convert to lower case\n",
    "\n",
    "    # text = replace_with(text, [('&amp;', 'and'), ('&gt;', '>'), ('&lt;', '<')])\n",
    "    for replace in replace_vals:\n",
    "        text = re.sub(replace[0], replace[1], text)\n",
    "\n",
    "    text = text.strip()  # remove leading and trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop_words):\n",
    "    '''function to remove stop words for given text'''\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text, tokenizer):\n",
    "    '''function to remove hashtags from the tweets'''\n",
    "    pattern = r'#'\n",
    "    joined_text = ' '.join(text)\n",
    "    cleaned_text = re.sub(pattern, '', joined_text)\n",
    "\n",
    "    return tokenizer.tokenize(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_length_one_words(text):\n",
    "    '''function to remove length one words'''\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    '''function to remove punctuation from the tweets'''\n",
    "    text = [word for word in text if word not in punctuation]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonalpha(text):\n",
    "    '''function to remove non alpha characters'''\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_twitter_data_set():\n",
    "    '''function to read the train dataset and print some information about the dataset amd rempove unneccessary columns'''\n",
    "    # read csv file into dataframe\n",
    "    twitter_hate_df = read_data_set('../data/twitter_hate.csv')\n",
    "    twitter_hate_df.drop([\"id\"], axis=1, inplace=True)  # remove id column\n",
    "\n",
    "    return twitter_hate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(twitter_hate_df):\n",
    "    '''function to cleanup teh tweets to prepare for training'''\n",
    "    # Normalize the casing, remove user handles and URLs.\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        pre_token_cleanup, args=(pre_token_replace_vals,))\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        tokenizer.tokenize)\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Remove stop words and redundant terms like ‘amp’, ‘rt’, etc..\n",
    "    stop_words = get_stop_words(\n",
    "        stop_words_filename, additional_stop_words)\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        remove_stop_words, args=(stop_words,))\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        remove_hashtags, args=(tokenizer,))\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Removing tweets with a length of 1.\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        remove_length_one_words)\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Removing punctuation.\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        remove_punctuation)\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    # Removing non-alpha characters.\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        remove_nonalpha)\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    return twitter_hate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_terms_in_tweets(twitter_hate_df):\n",
    "    '''function to find top 10 terms in tweets'''\n",
    "    # find top 10 terms in tweets\n",
    "    top_terms = Counter()\n",
    "    for tweet in twitter_hate_df[\"tweet\"]:\n",
    "        top_terms.update(tweet)\n",
    "\n",
    "    print(top_terms.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(twitter_hate_df):\n",
    "    '''function to format the data for training'''\n",
    "    # join the tokens back to string\n",
    "    twitter_hate_df[\"tweet\"] = twitter_hate_df[\"tweet\"].apply(\n",
    "        lambda x: ' '.join(x))\n",
    "    print(twitter_hate_df.head())\n",
    "\n",
    "    X = twitter_hate_df[\"tweet\"]\n",
    "    y = twitter_hate_df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_feature_vector(X_train, X_test):\n",
    "    '''function to transform the data to feature vector'''\n",
    "    # Create the TF-IDF vectorizer with the top 5000 terms.\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    print(X_train_vectorized.shape, X_test_vectorized.shape)\n",
    "\n",
    "    return X_train_vectorized, X_test_vectorized, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_vectorized, y_train):\n",
    "    '''function to train the model'''\n",
    "    # Train the model using the training data.\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_train_vectorized, X_test_vectorized):\n",
    "    '''function to make predictions'''\n",
    "    # Make predictions on the train data.\n",
    "    y_train_pred = model.predict(X_train_vectorized)\n",
    "    y_test_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "    return y_train_pred, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_train_pred, y_test_pred, y_train, y_test):\n",
    "    '''function to evaluate the model'''\n",
    "    # Evaluate the model using the training data.\n",
    "    print(\"Accuracy on training data:\", accuracy_score(y_train_pred, y_train))\n",
    "    print(\"Accuracy on test data:\", accuracy_score(y_test_pred, y_test))\n",
    "\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusting_class_imbalance_for_balance_model(X_train_vectorized, y_train):\n",
    "    '''function to adjust class imbalance'''\n",
    "    # Adjust class imbalance.\n",
    "    balanced_model = LogisticRegression(class_weight='balanced')\n",
    "    balanced_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    return balanced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_balanced_model(balanced_model, X_train_vectorized, X_test_vectorized):\n",
    "    '''function to make predictions with balanced model'''\n",
    "    # Make predictions on the train data.\n",
    "    y_train_pred = balanced_model.predict(X_train_vectorized)\n",
    "    y_test_pred = balanced_model.predict(X_test_vectorized)\n",
    "\n",
    "    return y_train_pred, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_balanced_model(y_train_pred, y_test_pred, y_train, y_test):\n",
    "    '''function to evaluate the model with balanced model'''\n",
    "    # Evaluate the model using the training data.\n",
    "    print(\"Accuracy on training data:\", accuracy_score(y_train_pred, y_train))\n",
    "    print(\"Accuracy on test data:\", accuracy_score(y_test_pred, y_test))\n",
    "\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_for_grid_search():\n",
    "    '''function to get parameters for grid search'''\n",
    "    # Get the parameters for grid search.\n",
    "    # parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    #               'penalty': ['l1', 'l2']}\n",
    "\n",
    "    parameters = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "                  'penalty': ['l1', 'l2']}\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_search_model(X_train_vectorized, y_train, parameters):\n",
    "    '''function to grid search model'''\n",
    "    # Grid search model.\n",
    "    grid_search = GridSearchCV(estimator=LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "                               param_grid=parameters,\n",
    "                               cv=StratifiedKFold(4),\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1,\n",
    "                               scoring='recall')\n",
    "\n",
    "    grid_search.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "    # print(grid_search.best_score_)\n",
    "    print(grid_search.best_estimator_)\n",
    "    # print(grid_search.cv_results_)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_grid_search_model(grid_search_model, X_train_vectorized, X_test_vectorized):\n",
    "    '''function to make predictions with grid search model'''\n",
    "    # Make predictions on the train data.\n",
    "    y_train_pred = grid_search_model.predict(X_train_vectorized)\n",
    "    y_test_pred = grid_search_model.predict(X_test_vectorized)\n",
    "\n",
    "    return y_train_pred, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_hate_df = read_twitter_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_hate_df = clean_tweets(twitter_hate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_terms_in_tweets(twitter_hate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = format_for_training(twitter_hate_df)\n",
    "X_train_vectorized, X_test_vectorized, vectorizer = transform_to_feature_vector(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred, y_test_pred = make_predictions(model, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_train_pred, y_test_pred, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_model = adjusting_class_imbalance_for_balance_model(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred, y_test_pred = make_predictions_with_balanced_model(balanced_model, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_with_balanced_model(y_train_pred, y_test_pred, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_model = get_grid_search_model(X_train_vectorized, y_train, get_parameters_for_grid_search())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred, y_test_pred = make_predictions_with_grid_search_model(grid_search_model, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1735d6009fec296ab35a6ff8d55b30cd473833f78753da2ded15c72f320c58f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
